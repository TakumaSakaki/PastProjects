{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第11回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**今回のnotebookは[Colaboratory](https://colab.research.google.com/?hl=ja)（以下Colab）での利用を想定しています。**  \n",
    "Colabで利用する場合は```ランタイム -> ランタイムのタイプを変更```からGPUを選択してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題. Deep Q-Network (DQN)によるCartPoleの攻略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. OpenAI Gymの環境構築と可視化 (CartPole)\n",
    "2. Q NetworkとTarget Networkの実装\n",
    "3. Experience Replayの実装\n",
    "4. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題. Deep Q-Network (DQN)によるCartPoleの攻略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. OpenAI Gymの環境構築と可視化 (CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習のトイタスクとしてよく利用されるのがCartpoleです。台車に棒が縦に乗った状態で始まり、倒れようとする棒を台車を左右に動かすことで倒れないように保つゲームです。   \n",
    "今回はOpenAI GymというOpenAIが開発している強化学習研究・開発のためのプラットフォームを利用して、 CartpoleをDQNで攻略します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、ランダムに行動を選択させて画面が出力されることを確認します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `gym.make('ゲームの名前')`: 環境を構築\n",
    "- `env.reset()`: エピソードを開始\n",
    "- `env.render()`: ゲーム画面の出力\n",
    "- `env.action_space.sample()`: 行動をランダムに選択\n",
    "- `next_state, reward, terminal, _ = env.step(action)`: 行動を環境に渡し, 次の状態, 報酬, エピソードの終了についての情報を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab上では以下を実行してください\n",
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!pip install JSAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab上では以下を実行してください\n",
    "from pyvirtualdisplay import Display\n",
    "pydisplay = Display(visible=0, size=(400, 300))\n",
    "pydisplay.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "\n",
    "env = gym.make('CartPole-v0') # ゲーム環境の構築\n",
    "frames = []\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        env.render() # ゲーム画面の出力\n",
    "\n",
    "        action = env.action_space.sample() # 行動をランダムに選択\n",
    "        next_state, reward, terminal, _ = env.step(action) # 行動を実行し、 次の状態、 報酬、 終端か否かの情報を取得\n",
    "        \n",
    "        screen = env.render(mode='rgb_array')\n",
    "        frames.append(screen)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の確認\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
    "patch = plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "  \n",
    "def animate(i):\n",
    "    patch.set_data(frames[i])\n",
    "    \n",
    "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() #画面出力の終了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態や行動は以下のように設定されています。\n",
    "- state: サイズ(4,)のnp.ndarray\n",
    "    - (カートの位置, カートの速度, ボールの角度, ボールの角速度)\n",
    "- action:\n",
    "    - 0: カートを左に移動させる\n",
    "    - 1: カートを右に移動させる\n",
    "- reward:\n",
    "    - （常に）1.0\n",
    "- terminal:\n",
    "    - False: エピソード継続\n",
    "    - True: エピソード終了 (ポールが倒れた)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Q NetworkとTarget Networkの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態は4次元ベクトル、行動の候補数は2つなので、4->16->16->16->2のユニットを持つQ NetworkとTarget Networkを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_states = 4\n",
    "n_actions = 2\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "x_state = tf.placeholder(tf.float32, [None, n_states])\n",
    "\n",
    "def original_network(x):\n",
    "    with tf.variable_scope('Original', reuse=tf.AUTO_REUSE):\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(x)\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
    "        y = tf.layers.Dense(units=n_actions, kernel_initializer=initializer)(h)\n",
    "    return y\n",
    "\n",
    "def target_network(x):\n",
    "    with tf.variable_scope('Target', reuse=tf.AUTO_REUSE):\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(x)\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
    "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
    "        y = tf.layers.Dense(units=n_actions, kernel_initializer=initializer)(h)\n",
    "    return y\n",
    "\n",
    "q_original = original_network(x_state)\n",
    "vars_original = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Original')\n",
    "\n",
    "q_target = target_network(x_state)\n",
    "vars_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Networkに対して、ネットワークの重みをコピーするオペレーションを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_ops = [var_target.assign(var_original) for var_target, var_original in zip(vars_target, vars_original)]\n",
    "copy_weights = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練オペレーションを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.placeholder(tf.float32, [None])\n",
    "x_action = tf.placeholder(tf.int32, [None])\n",
    "q_value = tf.reduce_sum(q_original * tf.one_hot(x_action, n_actions), axis=1)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(tf.subtract(t,q_value)))\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_ops = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Experience Replayの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replayを実現するため、経験した履歴を保存するReplayMemoryを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque([], maxlen = memory_size)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size).tolist()\n",
    "\n",
    "        state      = np.array([self.memory[index]['state'] for index in batch_indexes])\n",
    "        next_state = np.array([self.memory[index]['next_state'] for index in batch_indexes])\n",
    "        reward     = np.array([self.memory[index]['reward'] for index in batch_indexes])\n",
    "        action     = np.array([self.memory[index]['action'] for index in batch_indexes])\n",
    "        terminal   = np.array([self.memory[index]['terminal'] for index in batch_indexes])\n",
    "        \n",
    "        return {'state': state, 'next_state': next_state, 'reward': reward, 'action': action, 'terminal': terminal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を始める前にランダムに行動した履歴をReplayMemoryに事前に貯めておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 50000 #メモリーサイズ\n",
    "initial_memory_size = 500 #事前に貯める経験数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "replay_memory = ReplayMemory(memory_size)\n",
    "\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    while not terminal:\n",
    "        action = env.action_space.sample() # ランダムに行動を選択\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action) # 状態、報酬、終了判定の取得\n",
    "        \n",
    "        transition = {\n",
    "            'state': state,\n",
    "            'next_state': next_state,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "            'terminal': int(terminal)\n",
    "        }\n",
    "        replay_memory.append(transition) # 経験の記憶\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    if step >= initial_memory_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ε-greedy方策を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_start = 1.0\n",
    "eps_end = 0.1\n",
    "n_steps = 10000\n",
    "def get_eps(step):\n",
    "    return max(0.1, (eps_end - eps_start) / n_steps * step + eps_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各種ハイパーパラメータを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "target_update_interval = 1000 #重みの更新間隔\n",
    "batch_size = 32\n",
    "n_episodes = 300\n",
    "step = 0\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_episodesの数だけ学習を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    copy_weights.run() # 初期重みのコピー\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        terminal = False\n",
    "\n",
    "        total_reward = 0\n",
    "        total_q_max = []\n",
    "        while not terminal:\n",
    "            q = q_original.eval(feed_dict={x_state: state[None]}) # Q値の計算\n",
    "            total_q_max.append(np.max(q))\n",
    "\n",
    "            eps = get_eps(step) # εの更新\n",
    "            if np.random.random() < eps:\n",
    "                action = env.action_space.sample() # （ランダムに）行動を選択\n",
    "            else:\n",
    "                action = np.argmax(q) # 行動を選択\n",
    "            next_state, reward, terminal, _ = env.step(action) # 状態、報酬、終了判定の取得\n",
    "            reward = np.sign(reward)\n",
    "            total_reward += reward # エピソード内の報酬を更新\n",
    "\n",
    "            transition = {\n",
    "                'state': state,\n",
    "                'next_state': next_state,\n",
    "                'reward': reward,\n",
    "                'action': action,\n",
    "                'terminal': int(terminal)\n",
    "            }\n",
    "            replay_memory.append(transition) # 経験の記憶\n",
    "            \n",
    "            batch = replay_memory.sample(batch_size) # 経験のサンプリング\n",
    "            q_target_next = q_target.eval(feed_dict={x_state: batch['next_state']}) # ターゲットQ値の計算\n",
    "            t_value = batch['reward'] + (1 - batch['terminal']) * gamma * q_target_next.max(1)\n",
    "            \n",
    "            train_ops.run(feed_dict = {x_state: batch['state'], x_action: batch['action'], t: t_value}) # 訓練オペレーション\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if (step + 1) % target_update_interval == 0:\n",
    "                copy_weights.run() # 一定期間ごとに重みをコピー\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print('Episode: {}, Reward: {}, Q_max: {:.4f}, eps: {:.4f}'.format(episode + 1, total_reward, np.mean(total_q_max), eps))\n",
    "    \n",
    "    # 学習させたネットワークでTest\n",
    "    frames = []\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "\n",
    "    total_reward = 0\n",
    "    while not terminal:\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "\n",
    "        q = q_original.eval(feed_dict={x_state: state[None]})\n",
    "        action = np.argmax(q)\n",
    "\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    print('Test Reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の確認\n",
    "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
    "patch = plt.imshow(frames[0])\n",
    "plt.axis('off')\n",
    "    \n",
    "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # 画面出力の終了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目次",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
